---
title: "Holsenbeck_S_7"
author: "Stephen Synchronicity"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
always_allow_html: yes
header-includes:
   - \usepackage{dcolumn}
output: 
  html_document: 
    self_contained: yes
    css: C:\Users\Stephen\Documents\R\win-library\3.4\neuhwk\rmarkdown\templates\DA5030\resources\bootstrap.min.css
    highlight: zenburn
    keep_md: no
    theme: spacelab
    toc: no
---
```{r setup, include=FALSE}
# Knitr Options
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE,cache=TRUE, fig.align='center', fig.height=5.5, fig.width=8, tidy=TRUE, tidy.opts=list(width.cutoff=80))
library(knitr)
knit_print.data.frame = function(x, ...) {
    res = paste(c("", "", kable(x)), collapse = "\n")
    asis_output(res)
}
options(scipen=12)
# Attach dependencies
rmarkdown::html_dependency_jquery()
rmarkdown::html_dependency_bootstrap("spacelab")
rmarkdown::html_dependency_jqueryui()
# Make reproducible
set.seed(1)
# Load packages
req.packages <- c("tidyverse","dplyr","htmltools","magrittr")
for (q in seq_along(req.packages)) {
  suppressPackageStartupMessages(library(req.packages[q],character.only = T))
}
```
```{r 'Assignment',eval=F}
# This code will extract the assignment HTML and print the output formatted for this Rmd document. Set Assignment html below
# Use if assignment has blue font headers, and lists of questions
library(rvest)
Q <- xml2::read_html("https://da5030.weebly.com/assignment-7.html") %>% rvest::html_nodes(xpath="//font[contains(@color,'#24678d')]/ancestor::div[1]") %>% rvest::html_children()
Qs <- vector("list",sum(stringr::str_detect(Q,"Problem")))
for(i in seq_along(Q)){
 if(Q[i] %>% html_text() %>% stringr::str_detect("Problem")){
  n <- Q[i] %>% html_text() %>% stringr::str_extract("(?<=Problem\\s)\\d") %>% as.numeric
  Qs[[n]][['h1']] <- paste("#",rvest::html_text(Q[i]),"\n")
  print( Qs[[n]][['h1']])
 next}else if(rvest::html_attrs(Q[i]) %>% grepl("paragraph",.,ignore.case = T) & html_children(Q[i]) %>% html_attrs() %>% grepl("rgb\\(85",.,ignore.case = T)){
    Qs[[n]][['q']] <- paste("<div class='q'>",html_text(Q[i]),"</div>\n```{r  '",n,"'}\n```\n<p class='a'></p>\n\n",sep="")
 }else {next}
  
  if(n == length(Qs)){break}
}
# grep(pattern=substr(html_text(n),1,20),x=xml_parent(n),ignore.case = T)
# Qtext <- xml2::read_html("https://da5030.weebly.com/assignment-7.html") %>% rvest::html_nodes(xpath="//font[contains(@color,'#24678d')]/ancestor::div[1]/following-sibling::div[contains(@class,'paragraph')][1]")
# Q.form <- vector("list",length(Q))
# for (i in seq_along(Q)) {
#  Q.form[[i]] <- list(title=NA,Qs=NA)
#   Q.form[[i]][["title"]] <- Q[i] %>% rvest::html_node(css="font") %>% rvest::html_text() %>% paste("# ",.,"\n",sep="") 
#    if(length(Qtext)>0){
#     li <- xml2::xml_contents(Qtext[[i]]) %>% xml2::xml_children() %>% rvest::html_text()
#     for (l in seq_along(li)) {
#     Q.form[[i]][['Qs']][l] <- paste("## ",i,letters[l],"\n<div class='q'>",li[l],"\n</div>\n```{r '",i,letters[l],"'}\n```\n<p class='a'>\n</p>",sep="")
#     }
#    }else {
#      
#    }
# }
lapply(Qs, FUN="cat",sep='\n')
detach("package:rvest")
```
# Problem 1 

<div class='q'>Build an R Notebook of the concrete strength example in the textbook on pages 232 to 239. Show each step and add appropriate documentation.</div>
```{r  '1 Load Data'}
# ----------------------- Sat Mar 24 16:02:47 2018 ------------------------#
# Load Data
cs <- read.csv("concrete.csv")
str(cs)
# Evaluate normality
psych::pairs.panels(cs)
# About 5 look normal, while 4 look skew
# ----------------------- Sat Mar 24 16:20:23 2018 ------------------------#
# min max normalization function will work better because 4/9 features are not normally distributed.
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}
# Apply min-max normalizations to all columns. 
cs.norm <- as.data.frame(lapply(cs, normalize))
# Verify that it worked.
cs$strength %>% summary
cs.norm$strength %>% summary
# ----------------------- Sat Mar 24 16:30:37 2018 ------------------------#
# Test and training data sets
cs.train <- cs.norm[1:773, ]
cs.test <- cs.norm[774:1030, ]
# install.packages("neuralnet")
# ----------------------- Sat Mar 24 17:36:38 2018 ------------------------#
# Train a net with the default 1 neuron in the hidden layer

library(neuralnet)
cs.model <- neuralnet(formula = strength ~ cement + slag + ash + water + superplastic + coarseagg + fineagg + age, data = cs.train)
# Visualize the outcome
plot(cs.model)
# Age>cement>slag. SSE:~5.08
# Make a prediction
cs.pred <- compute(cs.model, cs.test[-9])
# Evaluate the Pearson correlation of the prediction with the actual
cor(cs.pred$net.result,cs.test$strength)
# ----------------------- Sat Mar 24 17:46:25 2018 ------------------------#
# Change the number of layers in an iterative fashion, determine optimal number of neurons in hidden layer
n <- 1:10
tune <- vector()
for (i in n) {
  cs.model <- neuralnet(formula = strength ~ cement + slag + ash + water + superplastic + coarseagg + fineagg + age, data = cs.train, hidden = i)
  cs.pred <- compute(cs.model, cs.test[-9])
  tune[i] <- cor(cs.pred$net.result,cs.test$strength)
}
# plot the relationship of neurons in hidden layer to the accuracy of the prediction
plot(x=1:length(tune),y=tune, type="b", main = "Number of neurons in hidden layer v Prediction Accuracy")
text(1:length(tune),tune,labels=round(tune,3),adj=c(1,0))
# Equivalent performance at 5 & 8, with 10 showing about ~1.5% improvement
# Top performancing model
c(which.max(tune),tune[which.max(tune)])
plot(cs.model)
# Yikes
# It looks like there are various algorithms with which computations can be made. We will try two of the other algorithms here and see how they perform
# runs <- expand.grid(algo=c('sag',  'slr'),
# n=c(8,10))
# ----------------------- Sat Mar 24 18:11:56 2018 ------------------------#
# To improve the speed at which it executes we can just compare errors
# mods <- vector("list",nrow(runs))
# for (i in 1:nrow(runs)) {
#     mods[[i]] <- neuralnet(formula = strength ~ cement + slag + ash + water + superplastic + coarseagg + fineagg + age, data = cs.train, hidden = runs[i,'n'], algorithm = runs[i,'algo'], threshold= .01)
#   tune[i] <- mods[[i]]$result.matrix['error',]
# }
# tune <- lapply(mods,function(x){
#   cs.pred <- compute(x,cs.test[-9])
#   cor(cs.pred$net.result,cs.test$strength)
#   })
# ----------------------- Sat Mar 24 18:42:49 2018 ------------------------#
# Only the first model seemed to work properly and provide an error.

```
<p class='a'>The weights in this implementation provide the user with a better understanding of how much correlation each attribute has with the response variable. I am interested to know how one might be able to customize the function in each neuron of the a hidden layer. To use stock indicators as an example, one neuron might be an RSI (relative strength indicator), one an SMA (simple moving average), another would be an DM+ - DM- (directional momentum up or down compared to one another), and lastly a parabolic SAR (stop and reverse). I suppose one could compute each value as a column in an extended timeseries object, convert that to a dataframe, and then run the neural net on the df.</p>


# Problem 2 

<div class='q'>Build an R Notebook of the optical character recognition example in the textbook on pages 249 to 257. Show each step and add appropriate documentation.</div>
```{r  '2'}
# ----------------------- Sat Mar 24 20:35:45 2018 ------------------------#
# Read Data
letters <- read.csv("letterdata.csv")
str(letters)
# divide into training and test data
letters.train <- letters[1:16000, ]
letters.test  <- letters[16001:20000, ]
# ----------------------- Sat Mar 24 20:42:34 2018 ------------------------#
# Create a model
library(kernlab)
svm.linear.time <- system.time({letter.classifier <- ksvm(letter ~ ., data = letters.train,
                          kernel = "vanilladot")
# The results of printing the object directly are quite verbose, so we will print the error here, which we can hopefully use to calibrate future models.
letter.classifier@error
cM <- caret::confusionMatrix(predict(letter.classifier, letters.test),letters.test$letter)})
cM$overall
# The prediction indicates an accuracy of ~84%
# ----------------------- Sat Mar 24 20:50:26 2018 ------------------------#
# Use a different kernel to improve accuracy
svm.radial.time <- system.time({letter.classifier.rbf <- ksvm(letter ~ ., data = letters.train, kernel = "rbfdot")
cM.rbf <- caret::confusionMatrix(predict(letter.classifier.rbf, letters.test),letters.test$letter)})
cM.rbf$overall
(cM.rbf$overall[1]-cM$overall[1])/cM$overall[1]
# About an 11% improvement in accuracy
svm.linear.time
svm.radial.time
(svm.radial.time[3]-svm.linear.time[3])/svm.linear.time[3]
# With a cost of about ~11x the time to compute. Worth taking into consideration if we were to use larger datasets.
```
<p class='a'>Let's set up an optimization using caret.</p>
```{r '2 caret',eval=F}
library(caret)
ltr.train <- createDataPartition(letters$letter,times=1,p=.8)
ltr.train <- trainControl(method="repeatedcv",repeats=1,index=ltr.train, verboseIter = T, allowParallel = T)

# svmRadial.time <- system.time(ltr.mod <- train(letter~.,data=letters,method="svmRadial",tuneLength=9,metric="Kappa",trCtrl=ltr.train))

library(doParallel)
# make a cluster with all possible threads (not cores)
cl <- makeCluster(detectCores()-1)
# register the number of parallel workers (here all CPUs)
registerDoParallel(cl)
# return number of parallel workers
getDoParWorkers() 
svmRadial.par.time <- system.time(ltr.par.mod <- train(letter~.,data=letters,method="svmRadial",tuneLength=8,metric="Accuracy",trCtrl=ltr.train))
# insert parallel calculations here
# stop the cluster and remove  Rscript.exe childs (WIN)
stopCluster(cl)
registerDoSEQ()
train.results <- list(Time=svmRadial.par.time,Model=ltr.par.mod)
save(train.results,file="svmRadial.RData")
```
```{r '2 svmRadial'}
load("svmRadial.RData")
svmRadial.par.time <- train.results$Time
ltr.par.mod <- train.results$Model
svmRadial.par.time
svmRadial.par.time[3]/3600 # Hours
svm.radial.time
# ----------------------- Tue Mar 27 14:28:03 2018 ------------------------#
# The time involved to train the model with caret took approximately 81 times as long
(svmRadial.par.time[3]-svm.radial.time[3])/svm.radial.time[3]
# The best tune
ltr.par.mod$bestTune
# Model validation
cM.par <- caret::confusionMatrix(predict(ltr.par.mod$finalModel,newdata=letters.test[,-1],type="response"),letters.test$letter)
cM.par$overall
# 99.5 % accuracy - possibly overfitted, but excellent accuracy nonetheless.
(cM.par$overall[1]-cM$overall[1])/cM$overall[1] # ~ 19% improvement over the linear model
(cM.par$overall[1]-cM.rbf$overall[1])/cM.rbf$overall[1] # ~7% improvement over the single train radial model
```



# Problem 3 

<div class='q'>Build an R Notebook of the grocery store transactions example in the textbook on pages 266 to 284. Show each step and add appropriate documentation.</div>
```{r  '3'}
# ----------------------- Tue Mar 27 14:58:23 2018 ------------------------#
# Load Data
library(arules)
groceries <- arules::read.transactions("groceries.csv", sep = ",")
summary(groceries)
# View the first 5
inspect(groceries[1:5])
# View ordered Item frequencies in the top quantile
groc.iF <- itemFrequency(groceries)
(groc.q3 <- groc.iF[groc.iF > quantile(groc.iF)[4]][order(groc.iF[groc.iF > quantile(groc.iF)[4]],decreasing=T)])
# Visualize the frequencies with support greater than 10%
itemFrequencyPlot(groceries, support = 0.1) # This matches the output above
# Generate a Pareto bar graph of the top 20 items with the most support
itemFrequencyPlot(groceries, topN = 20) 
# Visualize the sparse data matrix top 10
image(groceries[1:10])
# Useful for determining if there is some repetitive value (that's not a transaction item) in the dataset that shouldn't be there. Also useful for noticing seasonal patterns if the data is sorted by timeseries.
# Visualize 100 randomly selected rows
image(sample(groceries, 100)) # Vertical patterns (popular items) are notably easier to spot.
# Skipping the error run of apriori because thresholds are set at default and don't return any rules.
support(groceries,transactions = groceries) %>% quantile
# ----------------------- Tue Mar 27 15:41:40 2018 ------------------------#
# It looks like our third quantile of support begins at .013, so we will set our support threshold for the apriori there
sup.trshld <- support(groceries,transactions = groceries) %>% quantile %>% .[4]
# We will stick with the example exercise value for the confidence threshold
# minlen = 2 makes it such that at least 2 items must appear in a rule
groceryrules <- apriori(groceries, parameter = list(support =
                          sup.trshld, confidence = 0.25, minlen = 2))
groceryrules %>% summary # 101 rules
# Noteable is that with the higher threshold here, the rules that qualified contained at max 3 items.
# ----------------------- Tue Mar 27 16:01:23 2018 ------------------------#
# Inspect the rules with the top 10% of lift, sorted by lift

inspect(sort(groceryrules[groceryrules@quality[["lift"]] > qnorm(.9,mean(groceryrules@quality[["lift"]]),sd(groceryrules@quality[["lift"]]))],by="lift",decreasing = T))
# These are likely to be actionable
# Let's see what gets bought with vegetables or fruit
(vegfrurules <- subset(groceryrules,items %pin% c("fruit")|items %pin% c("vegetables")))
inspect(sort(vegfrurules[vegfrurules@quality[["lift"]] > qnorm(.90,mean(vegfrurules@quality[["lift"]]),sd(vegfrurules@quality[["lift"]]))],by="lift",decreasing = T))
# Convert to dataframe
vegfrurules %<>% as(Class="data.frame")
```
<p class='a'>It's interesting to learn about the formula behind the "Customers who bought this item also bought" feed on many online retailers. With regards to usage, subsetting takes some experimentation to achieve the expected results. I can see how association rules might also have usefulness in text-analysis showing words that are often associated in the same sentence with one another.</p>
