---
title: "Holsenbeck_S_1"
author: "Stephen Synchronicity"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
always_allow_html: yes
header-includes:
   - \usepackage{dcolumn}
output: 
  html_document: 
    self_contained: yes
    css: C:\Users\Stephen\Documents\R\win-library\3.4\neuhwk\rmarkdown\templates\DA5030\resources\bootstrap.min.css
    highlight: zenburn
    keep_md: no
    theme: spacelab
    toc: yes
    toc_float: true
---
```{r setup, include=FALSE}
# Knitr Options
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE,cache=TRUE, fig.align='center', fig.height=3.5, fig.width=5, tidy=TRUE, tidy.opts=list(width.cutoff=80))
library(knitr)
knit_print.data.frame = function(x, ...) {
    res = paste(c("", "", kable(x)), collapse = "\n")
    asis_output(res)
}
# Attach dependencies
rmarkdown::html_dependency_jquery()
rmarkdown::html_dependency_bootstrap("spacelab")
rmarkdown::html_dependency_jqueryui()
# Make reproducible
set.seed(1)
# Load packages
require("tidyverse")
require("dplyr")
require("htmltools")
require("rvest")
```
```{r 'Assignment',eval=T,results='asis'}
#Set Assignment html below
Q <- read_html("https://da5030.weebly.com/assignment-5.html") %>% html_nodes(xpath="//h2[contains(@class,'wsite-content-title')]/font[contains(@color,'#24678d')]")
Qtext <- read_html("https://da5030.weebly.com/assignment-5.html") %>% html_nodes(xpath="//h2[contains(@class,'wsite-content-title')]/font[contains(@color,'#24678d')]/parent::h2/following-sibling::div[contains(@class,'paragraph')][1]")
Q.form <- vector()
for (i in seq_along(Q)) {
  Q.form[i] <- Q[i] %>% html_text %>% paste("## ",.,"\n<div class='q'>",sep="") %>% paste(html_text(Qtext[i]),sep="") %>% paste("\n</div>\n<p class='a'>\n```{r '",i,"'}\n```\n</p>\n",sep="") %>%  str_split("\n")
}
sapply(Q.form, FUN="cat",sep='\n',simplify=T)
```

## Problem 1 (25 Pts)
<div class='q'>Build an R Notebook of the bank loan decision tree example in the textbook on pages 136 to 149. Show each step and add appropriate documentation.
</div>
<p class='a'>
```{r '1'}
# ------------------- Tue Feb 27 13:45:24 2018 --------------------#
# Read & explore data
credit <- read.csv("credit.csv")
str(credit)
# There are 21 Variables in our dataset, different from the 17 in the example.
# Load the example dataset
creditOrig <- read.csv("credit(1).csv")
# Find the common attributes
intersect(names(credit),names(creditOrig))
# The datasets share 11 variables
# Find the differing attributes
setdiff(names(credit),names(creditOrig))
# There are 10 attributes that are unique in the new dataset
# ------------------- Tue Feb 27 13:52:54 2018 --------------------#
# Take a look at feature tables
sapply(credit[,sapply(credit, is.factor)],table,simplify = T)
# ------------------- Tue Feb 27 14:00:21 2018 --------------------#
# Summarize numeric attributes
sapply(credit[,sapply(credit, is.numeric)],summary,simplify = T)
table(credit$default)
credit$default <- factor(credit$default,levels=c("n"="1","y"="2"),labels = c("n","y"))
levels(credit$default)
table(credit$dependents)
# ------------------- Tue Feb 27 14:16:20 2018 --------------------#
# Create test & Train sets
set.seed(123)
train_sample <- sample(1000, 900)
str(train_sample)
credit_train <- credit[train_sample, ]
credit_test  <- credit[-train_sample, ]
# Ensure we have a roughly striated sampling of the default cases
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
# caret::createDataPArtition could be used to make the test & train data sets have a nearly exact striated sample across the default attribute
# ------------------- Tue Feb 27 14:53:19 2018 --------------------#
# Load the library and build a decision tree
library(C50)
(credit_model <- C5.0(credit_train %>% select(-default), credit_train$default))
# View the decision tree
summary(credit_model)
# It looks like the model is 85% accurate
# ------------------- Tue Feb 27 15:11:46 2018 --------------------#
# Evaluate model performance
credit_pred <- predict(credit_model, credit_test)
caret::confusionMatrix(credit_pred,credit_test$default)
# In practice with the test set, the accuracy is 74%. The model made 19 type 1 errors misclassifying actual defaults, which could be costly when it comes to loans. 
# ------------------- Tue Feb 27 15:21:12 2018 --------------------#
# Improve the model by adding more trials for boosting purposes
(credit_model10 <- C5.0(credit_train %>% select(-default), credit_train$default,trials = 10))
# View the decision tree accuracy table minus all the decision tree output
summary(credit_model10) %>% .[['output']] %>% str_split(regex("\\n(?=Evaluation)",T),n=2) %>% unlist %>% .[2] %>% cat(sep="\n")
# summarize, then select the output, split output into two sections on the newline right before it starts the evaluation table, unlist the str_split list output, select the 2nd section IE everything after the split, use cat to print it respecting the newline seperators
# Evaluate model performance
credit_pred10 <- predict(credit_model10, credit_test)
caret::confusionMatrix(credit_pred10,credit_test$default)
# The accuracy improved by ~2% while the number of type 1 errors improved by 2.
# ------------------- Tue Feb 27 15:55:03 2018 --------------------#
# Building a cost matrix that weights false negatives regarding loan defaults as more costly, thus attempting to minimize the rate of these cases.
# Create a list of dimensions that mirrors a confusion matrix
matrix_dimensions <- list(c("n", "y"), c("n", "y"))
# Add the names
names(matrix_dimensions) <- c("predicted", "actual")
# Verify
matrix_dimensions

# build the matrix with no penalty for accurate predictions, 1x penalty for false positives (predicting applicants default, when they don't), and a 4x penalty for false negatives (predicting applicants don't default when they actually do).
(error_cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimensions))
# Using the costs in the build, differing from the book we're going to also include 10 trials.
(credit_cost <- C5.0(credit_train %>% select(-default), credit_train$default,trials = 10,costs=error_cost))
# View the decision tree accuracy table minus all the decision tree output
summary(credit_cost) %>% .[['output']] %>% str_split(regex("\\n(?=Evaluation)",T),n=2) %>% unlist %>% .[2] %>% cat(sep="\n")
credit_predcost <- predict(credit_cost, credit_test)
caret::confusionMatrix(credit_predcost,credit_test$default)
# To clearly compare the models
caret::confusionMatrix(credit_pred,credit_test$default)$table[1,2]
caret::confusionMatrix(credit_pred10,credit_test$default)$table[1,2]
caret::confusionMatrix(credit_predcost,credit_test$default)$table[1,2]

# Fail, the cost matrix did not create better metrics related to the false negative (type 1 error) rate. 
```
</p>

## Problem 2 (25 Pts)
<div class='q'>Build an R Notebook of the poisonous mushrooms example using rule learners in the textbook on pages 160 to 168. Show each step and add appropriate documentation.
</div>
<p class='a'>
```{r '2'}
# ------------------- Tue Feb 27 17:26:24 2018 --------------------#
# Load data and examine it
mushrooms <- read.csv("mushrooms.csv", stringsAsFactors = TRUE)
str(mushrooms)
# drop the veil_type feature
mushrooms$veil_type <- NULL
# examine the class distribution
table(mushrooms$type)
prop.table(table(mushrooms$type))
# ------------------- Tue Feb 27 17:34:11 2018 --------------------#
# Create a model using a single rule
library(rJava)
library(RWeka)
# train OneR() on the data
levels(mushrooms$odor)
fnames <- c('a'='anise','c'='creosote','f'='foul','l'='almond','m'='musty','n'='none','p'='pungent','s'='spicy','y'='fishy') #as a guide
(mushroom_1R <- RWeka::OneR(type ~ ., data = mushrooms))
# Looks like one should sniff a mushroom before eating it :D
summary(mushroom_1R)
# Accurate nearly 99% of the time, but that means that there's a few dead mycophiles if the sole criteria used for edibility is odor.
# ------------------- Tue Feb 27 18:35:58 2018 --------------------#
# Now using ripper
(mushroom_JRip <- RWeka::JRip(type ~ ., data = mushrooms))
summary(mushroom_JRip)
# Using 9 rules, all mushrooms can be classified. The book provides more descriptive rules that a mycophile might be able to use. The dataset used here has had all factor values reduced to a single letter which probably improves computational speed but reduces the legibility of the rules.
```
</p>

## Problem 3 (25 Pts)
<div class='q'>So far we have explored four different approaches to classification: kNN, Naive Bayes, C5.0 Decision Trees, and RIPPER Rules. Comment on the differences of the algorithms and when each is generally used. Provide examples of when they work well and when they do not work well. Add your comments to your R Notebook. Be specific and explicit; however, no code examples are needed.
</div>
<p class='a'>
All the approaches, kNN, Naive Bayes, Decision Trees, and RIPPER Rules do not make assumptions about the data (a key differentiating factor from regression based models - linear/logistic/polynomial etc). Naive Bayes does, however, assume that the attributes are independent.
kNN trains quickly because it does not build a model, but due to the use of a distance algorithm, the addition of dimensions to the testing data can add multiplicative layers of complexity and time to complete a prediction because the algorithm must iterate the distance algorithm over the entire training set for each new observation in the test data. 
kNN has regression builds specifically for a numerical response variable prediction, whereas Naive Bayes will use bin classification for numeric response variables, thus kNN will be more precise with a numeric response variable.
Naive Bayes builds a probability model based on the training data, that predictions for the test data can be rapidly computed from. Naive Bayes is best suited for data that has primarily nominal attributes. If numerical attributes are present, Naive Bayes fits Gaussian curves and uses the probablity density to calculate probability for the numeric variable. Thus, numerical data with a high degree of variance will make for less accurate models.
If the response variable is nominal/categorical, and the dataset is very large and classifications must be made quickly with a high degree of accuracy, Naive Bayes is probably the best algorithm. If the response variable is numeric, kNN is best suited. If one needs a high degree of precision, the dataset is small to moderately sized, and a classification does not need to be made particularly quickly, kNN is best suited. 
However, both kNN and Naive Bayes do not have particularly transparent explanatory value, and thus, if transparency or human readibility is necessary for the classification problem, look to decision trees and rule-based algorithms.
Decision trees and RIPPER Rules are both capable of classifying 100% of the data, however this also makes them prone to over-fitting on training data. RIPPER Rules can be more computationally complex than a Decision tree because it allows for the re-evaluation of cut rules as the model is built, where decision tree uses a divide and conquer method where cuts are static as the model grows (though subtree replacement might change some of these when employing multiple trials to create an optimal model). The divide and conquer method also makes linear cuts in the data, thus cannot take into account more complex relationships between multiple variables.
Both Decision Trees and rule-based algorithms are likely to be faster than kNN and Naive Bayes, but are also likely to be less accurate with complex data.
Decision trees are well-suited for data with multiple numeric attributes, whereas RIPPER is not ideal for multiple numeric attributes.
RIPPER will likely provide the most human legible output.
To summarize, 
If the task has a numeric response variable, precision is paramount, and time and computational resources are ample: kNN regression
If the task has a numeric or nominal response variable, precision is not as important but accuracy still is, time and computational resources are limited, and/or the dataset is large and complex: Naive Bayes
If the task has a nominal response variable, precision & accuracy are less important than transparency & legibility with legibility being paramount, time and computational resources are limited, and there are few to no numeric attributes: RIPPER
If the task has a nominal response variable, precision & accuracy are less important than transparency & legibility, time and computational resources are limited, and there are multiple numeric attributes: Decision Tree
</p>

## Problem 4 (25 Pts)
<div class='q'>Much of our focus so far has been on building a single model that is most accurate. In practice, data scientists often construct multiple models and then combine them into a single prediction model. This is referred to as a model ensemble. Two common techniques for assembling such models are boosting and bagging. Do some research and define what model ensembles are, why they are important, and how boosting and bagging function in the construction of assemble models. Be detailed and provide references to your research. You can use this excerpt from Kelleher, MacNamee, and D'Arcy, Fundamentals of Machine Learning for Predictive Data Analytics as a starting point. This book is an excellent resource for those who want to dig deeper into data mining and machine learning.
</div>
<p class='a'>
```{r '4'}
```
</p>